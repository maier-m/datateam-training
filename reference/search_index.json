[
["index.html", "Datateam miscellaneous ", " Datateam miscellaneous "],
["final-checklist-before-notifying-submitter-pi.html", "Final checklist before notifying submitter (PI)", " Final checklist before notifying submitter (PI) Descriptive title: what, where, when - provides enough information to understand the contents at a general scientific level (only first word and proper nouns capitalized, as in a journal article title; do not add a period at the end, it will auto-populate) Ex: “Active layer soil bulk density, moisture, carbon and nitrogen concentration and stable isotope data from Alaska borehole sites, 2015” Descriptive abstract: specifies purpose, data collected, and other applicable information (≥100 words) - provides an overview of the scientific context/project/hypotheses, how the data set fits into the larger project, a synopsis of the experimental or sampling design, and a summary of the data contents Bad example: - A low-resolution 3D model of a subglacial conduit derived from a photographic survey and structure-from-motion. Good example: - In this study, the locations of 193 old aerial photographs of northern Alaskan landscapes were re-photographed and assessed for changes in vegetation. The original photographs were taken over northern Alaska between 1948 and 1951, and the new photographs were taken between 1999 and 2003. The region covered by the original and repeat photographs stretches from the southern extent of the Brooks Range in the south to the Coastal Plain in the north, and from the Chukchi Sea in the west to the Canning River in the east. The original photographs were taken by the U.S. military as part of geologic reconnaissance and exploration, and the method used to acquire them was to fly both sides of a river valley while photographing the river and the facing valley slopes. Of the several thousand original photographs, only a fraction were repeated for the purpose of assessing vegetation change. Repeat photographs were selected for geographic coverage and to produce the greatest likelihood of detecting vegetation change. The original and repeat photographs were then scanned and stored in TIFF format. Individual image file sizes range from 5 to 60 MB each, and the total file size for the data set is approximately 11 GB. Data file types are registered DataOne formats and properly set in the EML (formatName) and the system metadata for ALL DATA OBJECTS See registered DataOne formats here Data files stored using proprietary software could disappear or no longer be accessible when the software is no longer available. Preserving data in text-based, open-source file formats ensures preservation of data for, theoretically, all time (ex: CSV instead of XLS). These transferrable file formats also reduce the possibility of information being “lost in translation”. fileName is properly set (including extension) in the system metadata for all objects (besides resource map) - do this by using set_file_name() Data file names: clear, but short descriptions without blank spaces (include extension) Ex: “Corvallis_VegBiodiv_2007.csv” Column headers (within data files): no spaces (use underscore or camelCase), attributeList adequately defines ALL column headers (including units) Cell etiquette: only one value (piece of information) per cell (for lat/long use decimal degrees) - attributeList should define ALL codes used (defining missing value codes is essential) Each data file has either a dataTable or an otherEntity entityNames are properly set in all dataTables and otherEntitys Download buttons in dataTables and otherEntitys download correct data files dataTables and otherEntitys must include physical - all physical sections match the information for the respective data object: objectName (including extension), size (including units), authentication (checksum), formatName (file format), and url (online distribution info links end in correct PID) Creator and contact roles are complete - First/Last Name, emails are essential - use ONE givenName slot for EACH first OR middle name, like so: &lt;individualName&gt; &lt;givenName&gt;Austin&lt;/givenName&gt; &lt;givenName&gt;Samuel&lt;/givenName&gt; &lt;surName&gt;Post&lt;/surName&gt; &lt;/individualName&gt; - Whenever they exist and are known, ORCID iDs should be included for all party types - enter iDs (in this format: &lt;userId directory=&quot;https://orcid.org/&quot;&gt;https://orcid.org/0000-0000-0000-0000&lt;/userId&gt;) in the userId slots. Note the “s” in “https”. See how to do this in an automated fashion here - Use the “references” tag to copy information for the same individual to multiple “party type”s (“creator”, “contact”, “associatedParty”), like so - Emails are important, phone #s and addresses are less critical Geographic and temporal ranges are sensible (start date preceeds end date) and appropriate to the study site and period; geogrpahic description is accurate Funding numbers are accurate and included NSF-funded projects can be searched here; we also accept non-NSF funded arctic data sets Methods: provide enough detail so that a reasonable scientist can replicate the study without needing to consult anyone nor any other resources Double check for garbled symbols, spelling, and gramatical errors If the data set is still private and awaiting PI and/or submitter review, be sure that the accessPolicys on ALL OBJECTS (metadata, resource map, data objects) grants read, write, and changePermission privileges to the PI’s and/or submitter’s ORCID iDs (in this format: “http://orcid.org/XXXX-XXXX-XXXX-XXXX”) - do this using set_access(). Make sure to OMIT the “s” after “http” in the ORCID. rightsHolder is properly set (in this format: “http://orcid.org/XXXX-XXXX-XXXX-XXXX”) to PIs on ALL OBJECTS (metadata, resource map, data objects) - do this using set_rights_holder() - this should be a last step After approval and DOI’ing, log out to ensure the landing page looks correct to average users. If there are discrepencies, ensure all versions (of all objects) following a public version, are also public. "],
["cyberduck-instructions.html", "Cyberduck instructions", " Cyberduck instructions To use Cyberduck to transfer local files onto the Datateam server: Open Cyberduck. Check that you have the latest version (Cyberduck -&gt; Check for Update…). If not, download and install the latest (you may need Jesse or Jeanette to enter a pw). Click “Open Connection”. From the drop-down, choose “SFTP (Secure File Transfer Protocol)”. Enter “datateam.nceas.ucsb.edu” for Server. Enter your username and password. Connect. From here, you can drag and drop files to and from the server. "],
["project-specific-steps.html", "Project-specific steps", " Project-specific steps Sometimes many datasets are associated with a larger project, such as the State of Alaska Salmon and People (SASAP) project. These datasets should be given additional project-specific information using eml@dataset@project. This will add pre-defined information including the project title, funding sources, and key personnel. You will also want to set access permissions to the project as well. If you are working on a SASAP dataset, prior to writing the eml and publishing the dataset you will set the project with this code: source(&#39;~/sasap-data/data-submission/Helpers/SasapProjectCreator.R&#39;) eml@dataset@project &lt;- sasap_project() Then, update the access permissions in the system metadata using set_rights_and_access. pkg &lt;- get_package(mn, resource_map_pid) set_rights_and_access(mn, unlist(pkg), &#39;CN=SASAP,DC=dataone,DC=org&#39;, permissions = c(&#39;read&#39;, &#39;write&#39;, &#39;changePermission&#39;)) Finally, go through the SASAP checklist to ensure that the package meets all of the project-specific requirements for publishing. "],
["edit-data-packages.html", "Edit data packages ", " Edit data packages "],
["create-a-resource-map.html", "Create a resource map", " Create a resource map If you are creating a new data package. You must create a resource_map. Resource_maps provide information about the resources in the data package (e.g. what data files should be included in the package, where is the metadata, etc.) To create a new resource map with an existing published metadata_pid and data_pids, use the following command. resource_map_pid &lt;- arcticdatautils::create_resource_map(mn, metadata_pid = metadata_pid, data_pids = data_pids) "],
["edit-sysmeta.html", "Edit sysmeta", " Edit sysmeta To edit the sysmeta of an object with a pid (data, metadata, resource_map, etc.) first load the sysmeta into R using the following command. sysmeta &lt;- dataone::getSystemMetadata(mn, pid) Then edit the sysmeta slots by using @ functionality. For example, to change the fileName use the following command. sysmeta@fileName &lt;- &#39;New File Name.csv&#39; Note that some slots cannot be changed by simple text replace (particularly the accessPolicy). There are various helper functions for changing the accessPolicy and rightsHolder such as datapack::addAccessRule (which takes the sysmeta as an input) or arcticdatautils::set_rights_and_access, which only requires a PID. In general, you most frequently need to use dataone::getSystemMetadata to change either the formatId or fileName slots. After you have changed the necessary slot, you can update the system metadata using the following command. updateSystemMetadata(mn, pid, sysmeta) Identifiers and sysmeta Importantly, changing the system metadata does NOT necessitate a change in the PID of an object. This is because changes to the system metadata do not change the object itself, they are only changing the description of the object (although ideally the system metadata is accurate when an object is first published). Additional resources For a more in-depth (and technical) guide to sysmeta, check out the DataOne documentation: System Metadata Data Types in CICore "],
["get-package-and-eml.html", "Get package and EML", " Get package and EML To update a package, you must load it into your R environment. After setting your node, use the following commands to load the package. rm_pid &lt;- &quot;your_resource_map_pid&quot; pkg &lt;- arcticdatautils::get_package(mn, rm_pid, file_names = TRUE) After loading the package, you can also load the eml file into R using the following command. eml &lt;- EML::read_eml(rawToChar(dataone::getObject(mn, pkg$metadata))) Tip to always have the most recent resource map. When editing resource_maps, you always want to be working with the most recent update. To ensure you have the most recent resource_map, you can use the following commands. rm_pid_original &lt;- &quot;your_original_resource_map_pid&quot; all_rm_versions &lt;- arcticdatautils::get_all_versions(mn, rm_pid_original) rm_pid &lt;- all_rm_versions[length(all_rm_versions)] "],
["publish-an-object.html", "Publish an object", " Publish an object Objects (data files, xml metadata files) can be published to a DataONE node using the function arcticdatautils::publish_object. To publish an object, you must first get the format_id of the object you want to publish. A few common format_ids are listed below. # .csv file format_id &lt;- &quot;text/csv&quot; # .txt file format_id &lt;- &quot;text/plain&quot; # metadata file format_id = format_eml() Most objects have format_ids that can be found here on the DataONE website. Metadata files (as shown above) use a special function to set the format_id. If the format_id is not listed at the DataONE website, you can set the format_id &lt;- NULL. Once you know the format_id you can publish your object using the commands below. path &lt;- &quot;path/to/your/file&quot; format_id &lt;- &quot;your/format_id&quot; pid &lt;- arcticdatautils::publish_object(mn, path = path, format_id = format_id) After publishing the object, the pid will need to be added to a resource_map by updating or creating a resource_map. Additionally, the rights and access for the object must be set. "],
["set-dataone-nodes.html", "Set DataONE nodes", " Set DataONE nodes Tell R which repo you want to work with by setting the Coordinating Node (cn) and Member Node (mn) using the appropriate code below. A note on nodes - be very careful about what you publish on production nodes (PROD, or arcticdata.io). These nodes should NEVER be used to publish test or training datasets. Test nodes # ADC (test.arcticdata.io) cn &lt;- dataone::CNode(&#39;STAGING&#39;) mn &lt;- dataone::getMNode(cn,&#39;urn:node:mnTestARCTIC&#39;) # KNB (dev.nceas.ucsb.edu) cn &lt;- dataone::CNode(&quot;STAGING2&quot;) mn &lt;- dataone::getMNode(cn, &quot;urn:node:mnTestKNB&quot;) Production nodes # ADC (arcticdata.io) cn &lt;- dataone::CNode(&#39;PROD&#39;) mn &lt;- dataone::getMNode(cn,&#39;urn:node:ARCTIC&#39;) # KNB (knb.ecoinformatics.org) cn &lt;- dataone::CNode(&quot;PROD&quot;) mn &lt;- dataone::getMNode(cn, &quot;urn:node:KNB&quot;) More DataONE nodes can be found here. "],
["set-rights-and-access.html", "Set rights and access", " Set rights and access One final step when creating/updating packages is to make sure that the rights and access on all the objects that were uploaded are set correctly within the sysmeta. The function arcticdatautils::set_rights_and_access will set both, and arcticdatautils::set_access will just set access. There are two functions for this because a rights holder should always have access, but not all people who need access are rights holders. The rights holder of the data package is typically the submitter (if the dataset is submitted through the web form, sometimes referred to as the “registry”), but if a data team member is publishing objects for a PI, the rights holder should be the main point of contact for the dataset (i.e. the person who requested that we upload the data for them). To set the rights and access for all of the objects in a package, first get the ORCID of the person you want to give access and rights to. You can set this manually, or grab it from one of the creators in an eml file. You can look up ORCID here. # Manually set ORCiD subject &lt;- &#39;http://orcid.org/PUT0-YOUR-ORCD-HERE&#39; # Set ORCiD from eml creator subject &lt;- eml@dataset@creator[[1]]@userId[[1]]@.Data subject &lt;- sub(&quot;^https://&quot;, &quot;http://&quot;, subject) Note, when setting metadata, the ORCID must start with http://. ORCIDs in eml should start with https://. The sub command above will change this formatting for you. Next, set the rights and access using the following command. set_rights_and_access(mn, pids = c(pkg$metadata, pkg$data, pkg$resource_map), subject = subject, permissions = c(&#39;read&#39;,&#39;write&#39;,&#39;changePermission&#39;)) If you ever need to remove/add public access to your package or object, you can use remove_public_read or set_public_read, respectively. arcticdatautils::remove_public_read(mn, c(pkg$metadata, pkg$data, pkg$resource_map)) "],
["show-indexing-status.html", "Show indexing status", " Show indexing status Sometimes it takes a while for the website to be updated with the updates you’ve made in R. To check whether it’s been indexed, just use: show_indexing_status(mn, pids) The status bar will either show 0% (not indexed) or 100% (should be online already). "],
["update-a-package.html", "Update a package", " Update a package To update a package, use arcticdatautils::publish_update. This function has an argument for adding data PIDs (or otherwise including existing data PIDs to make sure that they stay with the package). This function allows you to add or remove data objects, and/or make metadata edits. First make sure you have the package you want to update, loaded into R. Updating metadata If you need to make edits to the metadata, do so. Then write the eml metadata to a file. eml_path &lt;- &quot;path/to/metadata/science_metadata.xml&quot; write_eml(eml, eml_path) Next verify the eml is valid. EML::eml_validate(eml_path) The above should return TRUE if the eml is valid. If it returns FALSE, fix the eml file, use write_eml() to save the fixed file, and retry. Updating data If you need to make edits to the data, do so. Publish update Finally, update your data package at the Member Node. update &lt;- arcticdatautils::publish_update(mn, metadata_pid = pkg$metadata, resource_map_pid =pkg$resource_map, data_pids = pkg$data, child_pids = NULL, use_doi = FALSE, metadata_path = eml_path, public = FALSE, check_first = TRUE) If a package is ready to be public, you can change the public argument in the publish_update call to TRUE. If you want to publish with a DOI (Digital Object Identifier) instead of a UUID (Universally Unique Identifier), you can change the use_doi argument to TRUE. This should only be done after the package is finalized and has been thoroughly reviewed! If the package has children change child_pids to pkg$child_packages. "],
["update-an-object.html", "Update an object", " Update an object The arcticdatautils::update_object function updates a data object but does not change the resource map or EML. To simultaneously update all three parts of a data package, you can use the datamgmt::update_package_object. The update_package_object function updates a data object, and then automatically updates the package resource map with the new data pid. If an object already has a dataTable or otherEntity with a working physical section, the EML will be updated with the new data’s physical. To use, input the following arguments: update_package_object(mn, data_pid = &quot;the data pid you need to update&quot;, new_data_path = &quot;path/to/the/new/data.csv&quot;, resource_map_pid = &quot;resource map of the data package&quot;, format_id = &quot;the formatId&quot;, public = FALSE, use_doi = FALSE, ...) #any other arguments to pass to publish_update (child_pids, etc) To try it out for yourself, you can run this code to create a dummy package on test.arctic.io and update one of its objects. #input token to run cnTest &lt;- dataone::CNode(&#39;STAGING&#39;) mnTest &lt;- dataone::getMNode(cnTest,&#39;urn:node:mnTestARCTIC&#39;) pkg &lt;- create_dummy_package2(mnTest, title = &quot;My package&quot;) file.create(&quot;new_file.csv&quot;) update_package_object(mnTest, pkg$data[1], &quot;new_file.csv&quot;, pkg$resource_map, format_id = &quot;text/csv&quot;) file.remove(&quot;new_file.csv&quot;) #clean up #check out the package on test.arcticdata.io You will need to be explicit about your format_id here based on the file type. A list of format IDs can be found here on the DataONE website. "],
["explore-eml.html", "Explore EML", " Explore EML Access specific elements The eml_get function is a powerful tool for exploring EML (more on that here). It takes any chunk of eml and returns all instances of the element you specify. Note: you’ll have to specify the element of interest exactly, according to the spelling/capitalization conventions used in EML. Here are some examples: # Load an example EML file eml &lt;- EML::read_eml(system.file(&quot;example-eml.xml&quot;, package = &quot;arcticdatautils&quot;)) # Try these to see get various components of the eml file EML::eml_get(eml, &quot;creator&quot;) EML::eml_get(eml, &quot;boundingCoordinates&quot;) EML::eml_get(eml, &quot;url&quot;) You can also use the which_in_eml function from the datamgmt package to get indices within an eml list. Here are some examples: # Question: Which creators have a surName &quot;Smith&quot;? n &lt;- which_in_eml(eml@dataset@creator, &quot;surName&quot;, &quot;Smith&quot;) # Answer: eml@dataset@creator[n] # Question: Which dataTables have an entityName that begins with &quot;2016&quot; n &lt;- which_in_eml(eml@dataset@dataTable, &quot;entityName&quot;, function(x) {grepl(&quot;^2016&quot;, x)}) # Answer: eml@dataset@dataTable[n] # Question: Which attributes in dataTable[[1]] have a numberType &quot;natural&quot;? n &lt;- which_in_eml(eml@dataset@dataTable[[1]]@attributeList@attribute, &quot;numberType&quot;, &quot;natural&quot;) # Answer: eml@dataset@dataTable[[1]]@attributeList@attribute[n] #&#39; # Question: Which dataTables have at least one attribute with a numberType &quot;natural&quot;? n &lt;- which_in_eml(eml@dataset@dataTable, &quot;numberType&quot;, function(x) {&quot;natural&quot; %in% x}) # Answer: eml@dataset@dataTable[n] "],
["navigate-through-eml.html", "Navigate through EML", " Navigate through EML The first task when editing an eml file is navigating the eml file. An eml file is organized in a structure that contains many lists nested within other lists. The function EML::eml_view (make sure to install the listviewer if you don’t have it already) allows you to get a crude view of an eml file in the viewer. It can be useful for exploring the file. To navigate this complex structure in R, use the @ symbol. The @ symbol allows you to go deeper into the eml structure and to see what slots are nested within other slots. However, you have to tell R where you want to go in the structure when you use the @ symbol. For example, if you want to go into the dataset of your eml you would use the command eml@dataset. If you want to go to the creators of your data set you would use eml@dataset@creator. Note here that creators are contained within dataset. If you aren’t sure where you want to go, hit the tab button on your keyboard after typing @ and a list of available locations in the structure will appear (e.g., eml@&lt;TAB&gt;): RStudio Autocompletion Example Note if you hit tab, and the only option is .Data, this implies most likely that you are trying to go deeper within a list. For example eml@dataset@creator@&lt;TAB&gt; will return only .Data. This is because creator is a list object (i.e. you can have multiple creators). If you want to go deeper into creator, you first must tell R which creator you are interested in. Do this by writing [[i]] first where i is index of the creator you are concerned with. For example, if you want to look at the first creator i = 1. Now eml@dataset@creator[[1]]@&lt;TAB&gt; will give you many more options. Note, .Data also sometimes means you have reached the end of a branch in the eml structure. "],
["understand-the-eml-schema.html", "Understand the EML schema", " Understand the EML schema Another great resource for navigating the eml structure is looking at the schema which defines the structure. The .png files on this page show the schema as a diagram. Additional information on the schema and how different elements are defined can be found here). However, the schema is complicated and may take some time to get familiar with before you will be able to fully understand it. For example, let’s take a look at eml-party. To start off, notice that some elements are in solid boxes, whereas others are in dashed boxes. A solid box indicates that the element is required if the element above it (to the left in the schema) is used, whereas a dashed box indicates that the element is optional. Notice also that below the givenName element it says “0..infinity”. This means that the element is unbounded — a single party can have many given names and there is no limit on how many you can add. However, this text does not appear for the surName element — a party can have only one surname. You will also see icons linking the eml slots together which indicate the ordering of subsequent slots. These can indicate either a “sequence” or a “choice”. In our example from eml-party, a “choice” icon indicates that either an individualName, organizationName, or positionName is required, but you do not need all three. However, the “sequence” icon tells us that if you use an individualName, you must include the surName as a child element. If you include the optional child elements salutation and givenName, they must be written in the order presented in the schema. The eml schema sections you may find particularly helpful include eml-party, eml-attribute, and eml-physical. For a more detailed description of the eml schema, see the reference section on exploring EML. "],
["edit-eml.html", "Edit EML ", " Edit EML "],
["edit-an-eml-element.html", "Edit an EML element", " Edit an EML element There are multiple ways to edit an EML element. Edit EML with strings The most basic way to edit an EML element would be to go into the eml schema to the location of a character string and then replace that character string with a new character string. For example to change the title one could use the following commands. new_title &lt;- &quot;New Title&quot; eml@dataset@title[[1]]@.Data &lt;- new_title However, this isn’t the best method to edit the EML unless you are an expert both in S4 objects and in the EML schema, since the nesting and lists of elements can get very complex. Edit EML with EML For editing simple text sections, a better option is to use EML::read_eml. To use this function, one would first run the slot that is in need of editing. For example, for a title this would involve calling eml@dataset@title[[1]] and then copying the result. In this case, the result will be of the form &lt;title&gt;Title Text Here&lt;/title&gt;. To make a new title, one would replace the text between the &lt;title&gt;&lt;/title&gt; tags using a similar workflow as below. new_title &lt;- EML::read_eml(&quot;&lt;title&gt;New Title 2&lt;/title&gt;&quot;) eml@dataset@title[[1]] &lt;- new_title Note, without specifying which title with [[1]], the following code will give you the error Error in (function (cl, name, valueClass) : assignment of an object of class “title” is not valid for @‘title’ in an object of class “dataset”; is(value, &quot;ListOftitle&quot;) is not TRUE. # Bad Example new_title &lt;- EML::read_eml(&quot;&lt;title&gt;New Title 2&lt;/title&gt;&quot;) eml@dataset@title &lt;- new_title The above gives an error because eml@dataset@title is a slot for a list and new_title is a single object. Therefore you must either specify which title you want to replace as was done above by specifying the first title in the list with [[1]] or turn new_title into a list/vector utilizing the c() command as follows. new_title &lt;- EML::read_eml(&quot;&lt;title&gt;New Title 2&lt;/title&gt;&quot;) eml@dataset@title &lt;- c(new_title) However, if there were multiple titles, the above would replace all the titles with the single title. This behavior may or may not be desirable so be careful. One final note, a benefit of this method to edit EML objects is that advanced text features can be easily added using this workflow. Edit EML with objects A final way to edit an EML element would be to build a new object to replace the old object. To begin, you must determine the class of the object you want to edit (generally this is just the schema name of the object). The function class() is helpful here. For example, if you want to edit eml@dataset@title[[1]] use the following command to find the class. class(eml@dataset@title[[1]]) The result shows that this object has a class title. Therefore you must replace it with an object of class title. To do so use as(). To use as() input the the desired string followed by the desired class. new_title &lt;- as(&quot;New Title 3&quot;, &quot;title&quot;) eml@dataset@title[[1]] &lt;- new_title #or eml@dataset@title &lt;- c(new_title) Note, if you want to create an object with nested objects, you may have to use the command new() which is similar to as() but with the order of specifying values and class switched. See help on editing datatables for an example of when to use new(). "],
["edit-attributelists.html", "Edit attributeLists", " Edit attributeLists Attributes are stored in an attributeList. When editing attributes in R, you need to create one-three objects: A data.frame of attributes A data.frame of custom units (if applicable) A data.frame of factors (if applicable) Note, to edit/examine an existing attribute table already in a eml file you can use the following commands. attributeList &lt;- EML::get_attributes(eml@dataset@dataTable[[i]]@attributeList) attributes &lt;- attributeList$attributes Edit Attributes Attribute information should be stored in a data.frame with the following columns. attributeName: The name of the attribute as listed in the csv. Required. e.g.: “c_temp” attributeLabel: A descriptive label that can be used to display the name of an attribute. It is not constrained by system limitations on length or special characters. Optional. e.g.: “Temperature (Celsius)” attributeDefinition: Longer description of the attribute, including the required context for interpreting the attributeName. Required. e.g.: “The near shore water temperature in the upper inter-tidal zone, measured in degrees Celsius.” measurementScale: One of: nominal, ordinal, dateTime, ratio, interval. Required. nominal: unordered categories or text. e.g.: (Male, Female) or (Yukon River, Kuskokwim River) ordinal: ordered categories. e.g.: Low, Medium, High dateTime: date or time values from the Gregorian calendar. e.g.: 01-01-2001 ratio: measurement scale with a meaningful zero point in nature. Ratios are proportional to the measured variable. e.g.: 0 Kelvin represents a complete absence of heat. 200 Kelvin is half as hot as 400 Kelvin. 1.2 meters per second is twice as fast as 0.6 meters per second. interval: values from a scale with equidistant points, where the zero point is arbitrary. This is usually reserved for degrees Celsius or Fahrenheit, or latitude and longitude coordinates, or any other human-constructed scale. e.g.: there is still heat at 0° Celsius; 12° Celsius is NOT half as hot as 24° Celsius domain: One of: textDomain, enumeratedDomain, numericDomain, dateTimeDomain. Required. textDomain: text that is free-form, or matches a pattern enumeratedDomain: text that belongs to a defined list of codes and definitions. e.g.: CASC = Cascade Lake, HEAR = Heart Lake dateTimeDomain: dateTime attributes numericDomain: attributes that are numbers (either ratio or interval) formatString: Required for dateTimeDomain, NA otherwise. Format string for dates, e.g. “MM/DD/YYYY”. definition: Required for textDomain, NA otherwise. Definition for attributes that are a character string, matches attribute definition in most cases. unit: Required for numericDomain, NA otherwise. Unit string. If the unit is not a standard unit, a warning will appear when you create the attribute list, saying that it has been forced into a custom unit. Use caution here to make sure the unit really needs to be a custom unit. A list of standard units can be found here: https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/./eml-unitTypeDefinitions.html#StandardUnitDictionary numberType: Required for numericDomain, NA otherwise. Options are “real”, “natural”, “whole”, “integer”. real: positive and negative fractions and non fractions (…-1,-0.25,0,0.25,1…) natural: non-zero positive counting numbers (1,2,3…) whole: positive counting numbers and zero (0,1,2,3…) integer: positive and negative counting numbers and zero (…-2,-1,0,1,2…) missingValueCode: Code for missing values (e.g.: ‘-999’, ‘NA’, ‘NaN’). NA otherwise. Note that an NA missing value code should be a string, ‘NA’, and numbers should also be strings, ‘-999.’ missingValueCodeExplanation: Explanation for missing values, NA if no missing value code exists. You can create attributes by hand by typing them out in R following a workflow similar to that as below. attributes &lt;- data.frame( attributeName = c(&#39;Date&#39;, &#39;Location&#39;, &#39;Region&#39;,&#39;Sample_No&#39;, &#39;Sample_vol&#39;, &#39;Salinity&#39;, &#39;Temperature&#39;, &#39;sampling_comments&#39;), attributeDefinition = c(&#39;Date sample was taken on&#39;, &#39;Location code representing location where sample was taken&#39;,&#39;Region where sample was taken&#39;, &#39;Sample number&#39;, &#39;Sample volume&#39;, &#39;Salinity of sample in PSU&#39;, &#39;Temperature of sample&#39;, &#39;comments about sampling process&#39;), measurementScale = c(&#39;dateTime&#39;, &#39;nominal&#39;,&#39;nominal&#39;, &#39;nominal&#39;, &#39;ratio&#39;, &#39;ratio&#39;, &#39;interval&#39;, &#39;nominal&#39;), domain = c(&#39;dateTimeDomain&#39;, &#39;enumeratedDomain&#39;,&#39;enumeratedDomain&#39;, &#39;textDomain&#39;, &#39;numericDomain&#39;, &#39;numericDomain&#39;, &#39;numericDomain&#39;, &#39;textDomain&#39;), formatString = c(&#39;MM-DD-YYYY&#39;, NA,NA,NA,NA,NA,NA,NA), definition = c(NA,NA,NA,&#39;Sample number&#39;, NA, NA, NA, &#39;comments about sampling process&#39;), unit = c(NA, NA, NA, NA,&#39;milliliter&#39;, &#39;dimensionless&#39;, &#39;celsius&#39;, NA), numberType = c(NA, NA, NA,NA, &#39;real&#39;, &#39;real&#39;, &#39;real&#39;, NA), missingValueCode = c(NA, NA, NA,NA, NA, NA, NA, &#39;NA&#39;), missingValueCodeExplanation = c(NA, NA, NA,NA, NA, NA, NA, &#39;no sampling comments&#39;), stringsAsFactors = FALSE) However, typing this out in R can be a major pain. Luckily, there is a shiny app that you can use to build attribute information. You can use the app to build attributes from a data file loaded into R (recommended as the app will auto-fill some fields for you), or to edit an existing attribute table, or create attributes from scratch using the following commands (the commands will launch a shiny app in your web browser). # From data (recommended) datamgmt::create_attributes_table(data = data) # From an attribute table datamgmt::create_attributes_table(attributes_table = attributes_table) # From scratch datamgmt::create_attributes_table() Once you are done editing a table in the app, click the Print button to print text of a code that will build a data frame in R. Copy that code and assign it to a variable in your script (e.g. attributes &lt;- data.frame(...)). For simple attribute corrections, datamgmt::edit_attribute allows you to edit the slots of a single attribute within an attribute list. Edit Custom Units EML has a set list of units that can be added to an eml file. These can be seen by using the following code. standardUnits &lt;- get_unitList() View(standardUnits$units) If you have units that are not in the standard EML unit list, you will need to build a custom unit list. A unit typically consists of the following fields. id: The unit id (ids are camelCased) unitType: The unit type (run View(standardUnits$unitTypes) to see standard unitTypes) parentSI: The parent SI unit (e.g. for kilometer parentSI = “meter”) multiplierToSI: Multiplier to the parent SI unit (e.g. for kilometer multiplierToSI = 1000) abbreviation: Unit abbreviation (e.g. for kilometer abbreviation = “km”) description: Text defining the unit (e.g. for kilometer definition = “1000 meters”) Additionally, datamgmt::create_attributes_table will tell you if each of your units are standard or not. If your unit is not standard, you should use the following code to help auto-generate a custom unit. datamgmt::return_eml_units(&quot;your_unit&quot;) Note, datamgmt::create_attributes_table calls datamgmt::get_custom_units for you! Custom units need to have the following columns. datamgmt::get_custom_units will auto-generate many of these fields for you (but don’t just assume the autogeneration will be perfect, always ensure the autogeneration correctly handles your unit) Edit Factors For attributes that are enumeratedDomains, a table is needed with three columns: attributeName, code, and definition. attributeName should be the same as the attributeName within the attribute table and repeated for all codes belonging to a common attribute. code should contain all unique values of the given attributeName that exist within the actual data. definition should contain a plain text definition that describes each code. There is a tab in the datamgmt::create_attributes_table app that will help you build factors. If you need to build factors by hand, you can use named character vectors and then convert them to a data frame as shown in the example below. In this example, there are two enumerated domains in the attribute list - “Location” and “Region” Location &lt;- c(CASC = &#39;Cascade Lake&#39;, CHIK = &#39;Chikumunik Lake&#39;, HEAR = &#39;Heart Lake&#39;, NISH = &#39;Nishlik Lake&#39; ) Region &lt;- c(W_MTN = &#39;West region, locations West of Eagle Mountain&#39;, E_MTN = &#39;East region, locations East of Eagle Mountain&#39;) The definitions are then written into a data frame using the names of the named character vectors, and their definitions. factors &lt;- rbind(data.frame(attributeName = &#39;Location&#39;, code = names(Location), definition = unname(Location)), data.frame(attributeName = &#39;Region&#39;, code = names(Region), definition = unname(Region))) Finalize attributeList Once you have built your attributes, factors, and custom units, you can add them to eml objects. Attributes and factors are combined to form an attributeList using the following command. attributeList &lt;- EML::set_attributes(attributes = attributes, factors = factors) This attributeList must then be added to a dataTable. Custom units are added to additionalMetadata using the following command. unitlist &lt;- set_unitList(custom_units) eml@additionalMetadata &lt;- c(as(unitlist, &quot;additionalMetadata&quot;)) "],
["edit-datatables.html", "Edit dataTables", " Edit dataTables To edit a dataTable, first edit/create an attributeList and set the physical. Then create a new dataTable with the new() command as follows. dataTable &lt;- new(&quot;dataTable&quot;, entityName = &quot;A descriptive name for the data (does not need to be the same as the data file)&quot;, entityDescription = &quot;A description of the data&quot;, physical = physical, attributeList = attributeList) The dataTable must then be set to the eml i.e. eml@dataset@dataTable[[i]] &lt;- dataTable. "],
["edit-otherentities.html", "Edit otherEntities", " Edit otherEntities Remove otherEntities To remove an otherEntity use the following command. This may be useful if a data object is originally listed as an otherEntity and then transferred to a dataTable. eml@dataset@otherEntity[[i]] &lt;- NULL Create otherEntities If you need to create/update an otherEntitymake sure to publish or update your data object first (if it is not already on the DataONE node). Then build your otherEntity. otherEntity &lt;- arcticdatautils::pid_to_eml_other_entity(mn, pkg$data[[i]])[[1]] Alternatively, you can build the otherEntity of a data object not in your package by simply inputting the data pid. otherEntity &lt;- arcticdatautils::pid_to_eml_other_entity(mn, &quot;your_data_pid&quot;)[[1]] Next, give the otherEntity a name and description. otherEntity@entityName &lt;- as(&quot;A descriptive name for the data&quot;, &quot;entityName&quot;) otherEntity@entityDescription &lt;- as(&quot;A description of the data&quot;, &quot;entityDescription&quot;) The otherEntity must then be set to the eml i.e. eml@dataset@otherEntity[[i]] &lt;- otherEntity "],
["format-text-in-eml.html", "Format text in EML", " Format text in EML Currently, only certain fields (abstracts, methods) support text formatting in EML. Check out this demo for a full example. Additional info is also available here. Also note, you will need to edit an EML section with EML in order to use these workflows. Many of these formatting functions only work when enclosed by &lt;para&gt;&lt;/para&gt; Type-setting Subscripts, superscripts, and italics: &lt;subscript&gt;You can do subscripts&lt;/subscript&gt; &lt;superscript&gt;or superscipts&lt;/superscript&gt; &lt;emphasis&gt;or even italics.&lt;/emphasis&gt; Links Be sure to incluse the “https://” before the link or it will redirect incorrectly. Also, always check that your links go through to the correct page. Please be aware that most links are inherently unstable so always default to archiving files over pointing to websites when possible and appropriate. &lt;ulink url=&quot;https://some_url.com&quot;&gt; &lt;citetitle&gt;some text&lt;/citetitle&gt; &lt;/ulink&gt; Lists Unordered (bulletted) lists: &lt;itemizedlist&gt; &lt;listitem&gt; &lt;para&gt;Paragraphs&lt;/para&gt; &lt;/listitem&gt; &lt;listitem&gt; &lt;para&gt;Sections w/ subsections (w/ titles)&lt;/para&gt; &lt;/listitem&gt; &lt;/itemizedlist&gt; Ordered lists (1, 2, 3)… &lt;orderedlist&gt; &lt;listitem&gt; &lt;para&gt;something&lt;/para&gt; &lt;/listitem&gt; &lt;listitem&gt; &lt;para&gt;something else&lt;/para&gt; &lt;/listitem&gt; &lt;/orderedlist&gt; "],
["set-coverages.html", "Set coverages", " Set coverages Sometimes EML documents may lack coverage information describing the temporal, geographic, or taxonomic coverage of a data set. This example shows how to create coverage information from scratch, or replace an existing coverage element with an updated one. You can view the current coverage (if it exists) by entering eml@dataset@coverage into the console. Here the coverage, including temporal, taxonomic, and geographic coverages, is defined using set_coverage. coverage &lt;- EML::set_coverage(beginDate = &#39;2012-01-01&#39;, endDate = &#39;2012-01-10&#39;, sci_names = c(&#39;exampleGenus exampleSpecies1&#39;, &#39;exampleGenus ExampleSpecies2&#39;), geographicDescription = &quot;The geographic region covers the lake region near Eagle Mountain.&quot;, west = -154.6192, east = -154.5753, north = 68.3831, south = 68.3619) eml@dataset@coverage &lt;- coverage You can also set multiple geographic (or temporal) coverages. Here is an example of how you might set two geographic coverages. geocov1 &lt;- new(&quot;geographicCoverage&quot;, geographicDescription = &quot;The geographich region covers area 1&quot;, boundingCoordinates = new(&quot;boundingCoordinates&quot;, northBoundingCoordinate = new(&quot;northBoundingCoordinate&quot;, 68), eastBoundingCoordinate = new(&quot;eastBoundingCoordinate&quot;, -154), southBoundingCoordinate = new(&quot;southBoundingCoordinate&quot;, 67), westBoundingCoordinate = new(&quot;westBoundingCoordinate&quot;, -155))) geocov2 &lt;- new(&quot;geographicCoverage&quot;, geographicDescription = &quot;The geographich region covers area 2&quot;, boundingCoordinates = new(&quot;boundingCoordinates&quot;, northBoundingCoordinate = new(&quot;northBoundingCoordinate&quot;, 65), eastBoundingCoordinate = new(&quot;eastBoundingCoordinate&quot;, -155), southBoundingCoordinate = new(&quot;southBoundingCoordinate&quot;, 64), westBoundingCoordinate = new(&quot;westBoundingCoordinate&quot;, -156))) coverage &lt;- EML::set_coverage(beginDate = &#39;2012-01-01&#39;, endDate = &#39;2012-01-10&#39;, sci_names = c(&#39;exampleGenus exampleSpecies1&#39;, &#39;exampleGenus ExampleSpecies2&#39;)) eml@dataset@coverage@geographicCoverage &lt;- c(geocov1, geocov2) "],
["set-methods.html", "Set methods", " Set methods The methods tree in the EML section has many different options, visible in the schema. You can create new elements in the methods tree by following the schema and using the “new” command. Remember you can explore possible slots within an element by creating an empty object of the class you are trying to create. For example, method_step &lt;- new('methodStep'), and using auto-complete on method_step@. Potentially the most useful way to set methods is by editing with EML Another simple, and potentially useful way to add methods to an EML that have no methods at all is adding them via a word document. An example is shown below: methods1 &lt;- set_methods(&#39;methods_doc.docx&#39;) eml@dataset@methods &lt;- methods1 If you want to make minor changes to existing method information that has a lot of nested elements, your best bet may be to edit the EML manually in a text editor (or in RStudio), otherwise there is a risk of accidentally overwriting nested elements with blank object classes, therefore losing method information. "],
["set-parties.html", "Set parties", " Set parties To add people, with their addresses, you need to add addresses as their own object class, which you then add to the contact, creator, or associated party classes. NCEASadd &lt;- new(&quot;address&quot;, deliveryPoint = &quot;735 State St #300&quot;, city = &quot;Santa Barbara&quot;, administrativeArea = &#39;CA&#39;, postalCode = &#39;93101&#39;) The creator, contact, and associated party classes can easily be created using functions from the arcticdatautils package. Here, we use eml_creator to set our data set creator. JC_creator &lt;- arcticdatautils::eml_creator(&quot;Jeanette&quot;, &quot;Clark&quot;, &quot;NCEAS&quot;, &quot;jclark@nceas.ucsb.edu&quot;, phone = &quot;123-456-7890&quot;, userId = &quot;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&quot;, address = NCEASadd) eml@dataset@creator &lt;- c(JC_creator) Similarly, we can set the contacts. In this case, there are two, so we set eml@dataset@contact as a ListOfcontact, which contains both of them. JC_contact &lt;- arcticdatautils::eml_contact(&quot;Jeanette&quot;, &quot;Clark&quot;, &quot;NCEAS&quot;, &quot;jclark@nceas.ucsb.edu&quot;, phone = &quot;123-456-7890&quot;, userId = &quot;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&quot;, address = NCEASadd) JG_contact &lt;- arcticdatautils::eml_contact(&quot;Jesse&quot;, &quot;Goldstein&quot;, &quot;NCEAS&quot;, &quot;jgoldstein@nceas.ucsb.edu&quot;, phone = &quot;123-456-7890&quot;, userId = &quot;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&quot;, address = NCEASadd) eml@dataset@contact &lt;- c(JC_contact, JG_contact) Finally, the associated parties are set. Note that associated parties MUST have a role defined, unlike creator or contact. JG_ap &lt;- arcticdatautils::eml_associated_party(&quot;Jesse&quot;, &quot;Goldstein&quot;, &quot;NCEAS&quot;, &quot;jclark@nceas.ucsb.edu&quot;, phone = &quot;123-456-7890&quot;, address = NCEASadd, userId = &quot;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&quot;, role = &quot;metaataProvider&quot;) eml@dataset@associatedParty &lt;- c(JG_ap) "],
["set-physical.html", "Set physical", " Set physical To set the physical aspects of a dataTable, use the following commands to build a physical object from a data pid that exists in your package. physical &lt;- arcticdatautils::pid_to_eml_physical(mn, pkg$data[[i]]) Alternatively, you can get the physical of a data object not in your package by simply inputting the data pid. physical &lt;- arcticdatautils::pid_to_eml_physical(mn, &quot;your_data_pid&quot;) Note, the physical must then be added to a dataTable. A final, but not recommended option, is to set the physical by hand. To do so one can use a workflow similar to the one below. However, the far superior workflow is to publish or update your data first and then use `pid_to_eml_physical to set the physical id &lt;- &#39;your_data_pid&#39; #this should be an actual PID path &lt;- &#39;~/your/data/path&#39; #path to data table physical &lt;- EML::set_physical(objectName = &#39;your_file_name&#39;, id = id, size = as.character(file.size(path)), sizeUnit = &#39;bytes&#39;, authentication = digest(path, algo=&quot;sha1&quot;, serialize=FALSE, file=TRUE), authMethod = &#39;SHA-1&#39;, numHeaderLines = &#39;1&#39;, fieldDelimiter = &#39;,&#39;, url = paste0(&#39;https://cn.dataone.org/cn/v2/resolve/&#39;, id)) "],
["use-references.html", "Use references", " Use references Introduction References are a way to avoid repeating the same information multiple times in the same EML record. There are a few benefits to doing this, including: Making it clear that two things are the same (e.g., the creator is the same person as the contact, two entities have the exact same attributes) Reducing the size on disk of EML records with highly redundant information Faster read/write/validate with the R EML package You may want to use EML references if you have the following scenarios (not exhaustive): One person has multiple roles in the dataset (creator, contact, etc) One or more entities shares all or some attributes Example with parties It’s very common to see the contact and creator referring to the same person with eml like this (e.g. the creator is “Bryce Mecum” and the contact is “Bryce Mecum”). eml &lt;- new(&quot;eml&quot;) eml@packageId &lt;- new(&quot;xml_attribute&quot;, &quot;my_test_doc&quot;) eml@system &lt;- new(&quot;xml_attribute&quot;, &quot;my_system&quot;) eml@dataset@creator &lt;- c(arcticdatautils::eml_creator(&quot;Bryce&quot;, &quot;Mecum&quot;)) eml@dataset@contact &lt;- c(arcticdatautils::eml_contact(&quot;Bryce&quot;, &quot;Mecum&quot;)) So you see those two times Bryce Mecum is referenced there? If you mean to state that Bryce Mecum is the creator and contact for the dataset, this is a good start. But with just a name, there’s some ambiguity as to whether the creator and contact are truly the same person. Using references, we can remove all doubt. eml@dataset@creator[[1]]@id &lt;- new(&quot;xml_attribute&quot;, &quot;reference_id&quot;) eml@dataset@contact[[1]] &lt;- new(&quot;contact&quot;, reference = &quot;reference_id&quot;) print(eml) The reference id needs to be unique within the EML record but doesn’t need to have meaning outside of that. Example with attributes To use references with attributes: Add an attribute list to a data table Add a reference id for that attribute list Use references to add that information into the attributeLists of the other data tables For example, if all the data tables in our data package have the same attributes, we can set the attribute list for the first one, and use references for the rest: eml@dataset@dataTable[[1]]@attributeList &lt;- attribute_list eml@dataset@dataTable[[1]]@attributeList@id &lt;- new(&quot;xml_attribute&quot;, &quot;shared_attributes&quot;) for (i in 2:length(eml@dataset@dataTable)) { ref &lt;- new(&quot;references&quot;, &quot;shared_attributes&quot;) eml@dataset@dataTable[[i]]@attributeList@references &lt;- ref } Add creator IDs The datamgmt::add_creator_id() function is a shortcut for one of our most common use-cases: you get a package that does not have an ORCID associated with any of the contacts, and you need to add at least one into the metadata. To do this, you can: Add the ORCID to a creator (usually the first creator). You can look up ORCIDs here. Update the contact information for that individual’s other roles eml &lt;- eml %&gt;% datamgmt::add_creator_id(surname = &quot;mecum&quot;, orcid = &quot;https://orcid.org/0000-1234-5678-4321&quot;, id = &quot;bryce&quot;) # Use references to add updated contact info to Henrietta&#39;s other roles eml@dataset@contact[[1]] &lt;- new(&#39;contact&#39;, reference = &quot;bryce&quot;) eml@dataset@metadataProvider[[1]] &lt;- new(&#39;metadataProvider&#39;, reference = &quot;bryce&quot;) Updated creator information cannot be used as a reference for associatedParties because the extra “role” field is required. Also, the function does not (yet) account for cases in which multiple creators have the same surname. Check out the help file for add_creator_id for more information: ?datamgmt::add_creator_id "],
["pi-correspondence.html", "PI correspondence ", " PI correspondence "],
["email-templates.html", "Email Templates", " Email Templates Additional email templates Deadlines If the PI is checking about dates/timing: We process submissions in the order in which they are received, and yours still has a few ahead of it in our queue. Are you facing any deadlines? If so, we may be able to expedite publication of your submission. Pre-assigned DOI If the PI needs a DOI right away: We can provide you with a pre-assigned DOI that you can reference in your paper. However, please note that it will not become active until after we have finished processing your submission and the package is published. Best practices We noticed that the data files submitted are in _____ format. We recommend conversion of these files into a plain text/csv (OR ANOTHER APPROPRIATE OPEN-SOURCE) format in order to facilitate an accurate transfer of information to future researchers and ensure preservation of the data in perpetuity. We will perform these conversions for you. Below are some linked articles about data science best practices that the NSF Arctic Data Center adheres to: DataONE - https://www.dataone.org/best-practices “Some Simple Guidelines for Effective Data Management” - http://onlinelibrary.wiley.com/doi/10.1890/0012-9623-90.2.205/full “Good Enough Practices in Scientific Computing” - http://arxiv.org/pdf/1609.00037v1.pdf Adding metadata via R KNB does not support direct uploading of EML metadata files through the website (we have a webform that creates metadata), but you can upload your data and metadata through R! Here are some training materials we have that use both the dataone and datapack packages. It explains how to set your authentication token, build a package from metadata and data files, and publish the package to one of our test sites. I definitely recommend practicing on a test site prior to publishing to the production site your first time through. You can point to the KNB test node (dev.nceas.ucsb.edu) using this command: d1c &lt;- D1Client(&quot;STAGING2&quot;, &quot;urn:node:mnTestKNB&quot;) If you prefer, there are Java, Python, Matlab, and Bash/cURL clients as well. Nesting Nesting data sets under a common “parent” is beneficial so that all data sets funded by the same award can be found in one place. This way additional data sets can be added to the group without altering existing ones. Once we process all the “child” data sets you upload, we can group them under the same “parent”. The parent will not contain any data files itself, but will be populated with metadata only. Please view an example of a parent data set here. Would you like your submission(s) grouped with the other data sets funded by the same award? If so, we are happy to do all this grouping for you. Finding multiple data sets If linking to multiple data sets, you can send a link to the profile associated with the submitter’s ORCID ID and it will display all the data sets. Ex.: https://arcticdata.io/catalog/#profile/http://orcid.org/0000-0002-2604-4533 NSF ARC data submission policy The NSF ARC program managers check compliance with their data policies when checking annual reports. Generally, NSF ARC requires that fully quality controlled data be uploaded within 6 months of collection for AON projects, or within 2 years of collection (or by end of the grant) for other ARC funded projects. Additionally, complete metadata must be submitted within two years of collection or before the end of the award, whichever comes first. Please find an overview of the NSF ARC policies &gt;here and the full policy information &gt;here. Investigators should upload their data to the Arctic Data Center, or, where appropriate, to another community endorsed data archive that ensures the longevity, interpretation, public accessibility, and preservation of the data (e.g., GenBank, NCEI). Local and university web pages generally are not sufficient as an archive. Data preservation should be part of the institutional mission and data must remain accessible even if funding for the archive wanes (i.e., succession plans are in place). We would be happy to discuss the suitability of various archival locations with you further. In order to provide a central location for discovery of ARC-funded data, a metadata record must always be uploaded to the Arctic Data Center even when another community archive is used. Final email templates Asking for approval Hi PI, I have updated your data set and you can view it here after logging in: [URL] Please review and approve it for publishing or let us know if you would like anything else changed. For your convenience, if we do not hear from you within a week we will proceed with publishing. DOI and data set finalization comments Replying to questions about DOIs &gt;We attribute DOIs to data sets as one might give a DOI to a citable publication. Thus, a DOI is permanently associated with a unique and immutable version of a data set. If the data set changes, a new DOI will be created and the old DOI will be preserved with the original version. DOIs and URLs for previous versions of data sets remain active on the Arctic Data Center (will continue to resolve to the data set landing page for the specific version they are associated with), but a clear message will appear at the top of the page stating that “A newer version of this dataset exists” with a hyperlink to that latest version. With this approach, any past uses of a DOI (such as in a publication) will remain functional and will reference the specific version of the dataset that was cited, while pointing users to the newest version if one exists. Clarification of updating with a DOI and version control &gt;We definitely support updating a data set that has already been assigned a DOI, but when we do so we mark it as a new revision that replaces the original and give it its own DOI. We do that so that any citations of the original version of the data set remain valid (i.e.: after the update, people still know exactly which data were used in the work citing it). Sending finalized URL before resolving ticket [NOTE: the URL format is very specific here, please try to follow it exactly (but substitute in the actual DOI of interest)] &gt;Here is the link to your finalized data package: https://doi.org/10.00000/X00X0X Please let us know if you need any further assistance. Initial email template Hello _____, Thank you for your recent submission to the NSF Arctic Data Center! We will review your dataset and get back to you with any suggestions. From my preliminary examination of your submission I have noticed a few items that I would like to bring to your attention. We are here to help you publish your submission, but your continued assistance is needed to do so. See comments below: [COMMENTS HERE] Best, [YOUR NAME] Poor title Provides the what, where, and when of the data We would like to add some more context to your data package title. A descriptive title that provides context about the what, where, and when of a data package is often far more useful in search results. Something like: ‘OUR SUGGESTION HERE, WHERE, WHEN’ might be an option. Does not use acronyms We noticed that your title contains several acronyms or abbreviations. In order to increase discoverability of your title, please spell out all acronyms and abbreviations. Poor abstract Describes DATA in package (ideally &gt; 100 words) Your abstract, while informative, appears to be missing some information. We suggest that the abstract be sufficiently descriptive for a general scientific audience. It should provide an overview of the scientific context/ project/ hypotheses, how this data package fits into the larger project, a synopsis of the experimental or sampling design, and a summary of the data contents. If you prefer and it is appropriate, we could add language from the abstract in the NSF Award found here: [NSF AWARD URL]. Poor data At least one data file We noticed that no data files were submitted. With the exception of sensitive social science data, NSF requires the submission of all data products prior to publication. Do you intend to submit data files? No xls/xlsx files (or other proprietary files) We noticed that the data files submitted are in xlsx format. Please convert your files to a plain text/csv (or other open source format) in order to facilitate an accurate transfer of information to users and to ensure preservation of the data in perpetuity. We noticed you submitted your data as a .mat file. While the Arctic Data Center supports the upload of any data file format, sharing data can be greatly enhanced if you use ubiquitous, easy-to-read formats. We recommend conversion of your data to txt or csv (or other open) formats for better archival. File contents and relationships among files are clear Could you provide a short description of the files submitted? Information about how each file was generated (what software, source files, etc.) allows other scientists to reproduce your work. All attributes are clearly defined Check for descriptions of abbreviations and units: Could you describe ____? Please define “X”. Please define “XYZ”, including the unit of measure. What are the units of measurement for the columns labeled “ABC” and “XYZ”? Column names do not use spaces or special characters Data files should include column headers without special characters or spaces (i.e., Exp_no, NO3_M, Organic_M, as per “Some Simple Guidelines for Effective Data Management” - http://onlinelibrary.wiley.com/doi/10.1890/0012-9623-90.2.205/full). NA’s are defined. What do the NA’s in your measurements represent? (instrument failure, site not found, etc.) We noticed that the data files contain blank cells. What do these represent? Poor funding At least one funding number If there are multiple submissions with the same funding number/creators, check about nesting Thank you for your submission to the Arctic Data Center! Based on the NSF awards, your most recent submission and the [PACKAGE NAME] package appear to be related. Would you like your submission(s) grouped with the other data packages funded by the same award? If so, we are happy to do all this grouping for you. Thank you for your submission to the Arctic Data Center! Based on the NSF awards, your most recent submission and the [PACKAGE NAME] parent package seem like disparate projects, is that correct? Nesting data packages under a common “parent” is beneficial so that all packages funded by the same award can be found in one place. This way additional packages can be added to the group without altering existing ones. Once we process all the “child” data packages you upload, we can group them under the same “parent”. The parent does not need to contain any data files itself, but will be populated with metadata only. Poor methods Submissions should: provide instrument names specify how sampling locations were chosen provide citations for sampling methods that are not explained in detail Your methods, while informative, appear to be missing some information. Enough detail should be included so that a reasonable scientist can interpret the study and data for reuse without consulting you nor any other resources. This should hold true today, or even decades or a century from now. Users need to understand how the data were collected, how to interpret the values, and potentially how to use the data in the case of specialized files. We would be happy to add more content for you. Please provide us with a more robust methods section directly in this email or point us to a document from which we can extract more methods. NSF requires that comprehensive methods information be included directly in the metadata record. Pointers or URLs to other sites are unstable and insufficient. "],
["initial-review-checklist.html", "Initial review checklist", " Initial review checklist Before responding to a new submission use this checklist to review the submission. When your are ready to respond use the initial email template and insert comments as needed. Title Is descriptive of the work (provides enough information to understand the contents at a general scientific level) Provides a location of the work Provides a time frame of the work Abstract Describes the DATA Data At least one data file No xls/xlsx files (or other proprietary files) File contents and relationships among files are clear All attributes are clearly defined Column names do not use spaces or special characters NA’s are defined Funding At least one funding number If there are multiple submissions with the same funding number/creators, check about nesting Methods Enough detail is provided in metadata Contacts At least one contact with email and ORCiD Temporal/geographic coverage Includes coverage that makes sense. "],
["navigate-rt.html", "Navigate RT", " Navigate RT The RT ticketing system is how we communicate with folks interacting with the Arctic Data Center. We use it for managing submissions, accessing issues, etc. It consists of three separate interfaces: Front Page All Tickets Ticket Page Front page This is what you see first Home - brings you to this homepage Tickets - to search for tickets (also see number 5) Tools - not needed New Ticket - create a new ticket Search - Type in the ticket number to quickly navigate to a ticket Queue - Lists all of the tickets currently in arcticdata and their status New = unopened tickets that require attention Open = tickets currently open, under investigation by team member Stalled = tickets awaiting response from PI/Submitter Tickets I Own - These are the current open tickets that are claimed by me Unowned Tickets - Newest tickets awaiting claim Ticket Status - Status and how long ago it was created Take - claim the ticket as yours All tickets This is the queue interface from number 6 of the Front page 1. Ticket number and title 2. Ticket status 3. Owner - who has claimed the ticket Example ticket Title - Include the PI’s name for reference Display - homepage of the ticket History - Comment/Email history, see bottom of Display page Basics - edit the title, status and ownership here People - option to add more people to the watch list for a given ticket conversation. Note that user/ PI/ submitter email addresses should be listed as “Requestors”. Requestors are only emailed on “Replys”, not “Comments”. Ensure your ticket has a Requestor before attempting to contact users/ PIs/ submitters. Links - option to “Merge into” another ticket number if this is part of a larger conversation. Also option to add a reference to other ticket number Actions Reply - message the submitter/ PI/ all watchers Comment - attach internal message (no submitters, only Data Teamers) Open It - Open the ticket Stall - submitter has not responded in greater than 1 month Resolve - ticket completed History - message history and option to reply (to submitter and beyond) or comment (internal message) "],
["pi-faqs.html", "PI FAQs", " PI FAQs Q: Can I replace data that has already been uploaded and keep the DOI? A: Once you have published your data with the Arctic Data Center, it can still be updated by providing an additional version which can replace the original, while still preserving the original and making it available to anyone who might have cited it. To update your data, return to the data submission tool used to submit it, and provide an update. Any update to a data set qualifies as a new version and therefore requires a new DOI. This is because each DOI represents a unique, immutable version, just like for a journal article. DOIs and URLs for previous versions of data sets remain active on the Arctic Data Center (will continue to resolve to the data set landing page for the specific version they are associated with), but a clear message will appear at the top of the page stating that “A newer version of this data set exists” with a hyperlink to the latest version. With this approach, any past uses of a DOI (such as in a publication) will remain functional and will reference the specific version of the data set that was cited, while pointing users to the newest version if one exists. Q: Why don’t I see the data set that I uploaded to the ADC? Possible Answer #1: The data set is still private because we are awaiting your approval to publish it. Please login with your ORCID ID to view private data sets. Possible Answer #2: The data set is still private and you do not have access because you were not the submitter. If you need access please have the submitter send us a message from his/her email address confirming this, along with your ORCID ID. Once we receive that confirmation we will be happy to grant you permission to view and edit the data set. Possible Answer #3: The data set is still private and we accidentally failed to grant you access. I apologize for the mistake. I have since updated the access policy. Please let us know if you are still having trouble viewing this data set here: (URL). Remember to login with your ORCID ID. Issue: I would like to display multiple geographic coverages for my data set, but the form only accepts one point or bounding box. Unfortunately, the current web form does not allow users to input more than one geographic coverage. We are happy to add multiple coverages (maps) for you. Please provide us with the coordinates. Note that next version of the web form will support the addition of multiple locations by users. We anticipate this release in the coming months. Issue: MANY files to upload (100s or 1000s). A: Please consider zipping the files up for transfer. Can you upload the files to a drive we can access, such as G Drive or Dropbox? Alternatively, if you have a publicly accessible FTP you can point us to, we could grab the files from there. If needed, we have a secure FTP you can access. Details are available here. Please access our server at datateam.nceas.ucsb.edu with the username “visitor”. Let us know if you would like to use our SFTP and we will send you the password and the path to which directory to upload to. Q: May another person (e.g. my student) submit data using my ORCID ID so that it is linked to me? A: I would recommend instead that the student set up their own ORCID accounts at https://ORCiD.org/register and submit data sets from that account. Submissions are processed by our team and, at that point, we can grant you full rights to the metadata and data files even though another person submitted them. Issue: Web form not cooperating. A: I apologize that you are experiencing difficulties while attempting to submit your data set. We are happy to attempt to troubleshoot this for you. Which operating system (including the version) and browser (with version #) are you using? At which exact step did the issue arise. What error message did you receive? Please provide us with any screenshots of the error message. Q: May I submit a non-NSF funded data set? A: Yes, you can submit non-NSF-funded Arctic data if you are willing to submit under the licensing terms of the Arctic Data Center (CC-0 or CC-BY), the data are moderately sized (with exact limits open to discussion), and a lot of support time to curate the submission is not required (i.e., you submit a complete metadata record and well formatted, open-source data files). For larger data sets, we would likely need to charge a one-time archival fee which amortizes the long-term costs of preservation in a single payment. Also, please note that NSF-funded projects take priority when it comes to processing. Information on best practices for data and metadata organization is available here. Q: Can I add another data file to an existing submission without having to fill out another metadata form? A: Yes. Navigate to the data set after being sure to login. Then click the green “Edit” button. The form will populate with the already existing metadata so there is no need to fill it out again. Just scroll to the bottom of the form to “Upload Data”, click “Choose File”, and browse to the data file you wish to add. Be aware that the DOI will need to change after you add this file (or make any changes to a data set) as, just like for a journal article, a DOI represents a unique and immutable version. The current URL will remain functional, but clearly display a message at the top of that page stating “A newer version of this data set exists” with a link to the latest version. Q: Can I add these data anytime or is there some deadline associated with the grant, or some other restriction I’m not aware of? A: We are happy to accept your submission any time; however, NSF has stricter policies. For all ARC supported projects, the policies state that “Complete metadata must be submitted… within two years of collection or before the end of the award, whichever comes first. All data and derived data products that are appropriate for submission… must be submitted within two years of collection or before the end of the award, whichever comes first.” For all AON projects, “Real-time data must be made publicly available immediately. All data must be submitted… within 6 months of collection, and be fully quality controlled. All data sets and derived data products must be accompanied by a metadata profile and full documentation.” Q: Can you clarify what constitutes sensitive information (in relation to social science data and whether it needs to be uploaded)? A: Sensitive information includes human subjects data and data that are governed by an Institutional Review Board policy. Data that are ethically or legally sensitive or at risk of decontextualization also constitute sensitive information. The NSF policy states “NSF realizes that on occasion there are data gathered of a particularly sensitive nature, such as the locations of archaeological sites or nest locations of endangered species. It is not the intention of this policy to reveal such information publicly. Discipline standards, indigenous community cultural rules, and state and federal regulations and laws should be followed for these types of data.” The full policies are available here. Q: Can we submit data as an Excel file? A: While the Arctic Data Center supports the upload of any data file format, sharing data can be greatly enhanced if you use ubiquitous, easy-to-read formats. For instance, while Microsoft Excel files are commonplace, it is better to export these spreadsheets to Comma Separated Values (CSV) text files, which can be read on any computer without needing to have Microsoft products installed. Data submitted in Excel workbooks will undergo conversion to CSVs by our staff before being made public. So, yes, you are free to submit an Excel workbook, however we strongly recommend converting each sheet to a CSV. The goal is not only for users to be able to read data files, but to be able to analyze them with software, such as R Studio. Typically, we would extract any plots and include them as separate image files. [ONLY SAY THIS NEXT PART IF THE PI CONTINUES TO INSIST] I understand that having the plots in the same file as the data they are built from simplifies organization. If you definitely prefer to have the Excel workbook included, we ask that you allow us to document the data in both formats and include a note in the metadata clarifying that the data are indeed duplicated (but in different formats). "],
["miscellaneous-file-types.html", "Miscellaneous file types ", " Miscellaneous file types "],
["datalogger-files.html", "Datalogger files", " Datalogger files .hobo files are raw data files offloaded from a sensor, and can include real-time graphs of measured variables, such as temperature and soil moisture conditions. .hobo files can be opened using HOBOware, which can be downloaded from here. "],
["spatial-data.html", "Spatial data", " Spatial data Shapefiles and their associated files can be kept in a single grouping using *.zip (or another similar format). To extract meatadata, simply run: library(rgdal) root_folder_path &lt;- &quot;dir1/dir2/&quot; folder_name &lt;- &quot;folder_name&quot; #name of folder containing shapefiles and associated data spatial_obj &lt;- readOGR(dsn = paste0(folder_path, folder_name), layer = folder_name) spatial_df &lt;- spatial_obj@data #save data as a dataframe spatial_obj@proj4string #get coordinate unit information "],
["wrangle-data.html", "Wrangle data ", " Wrangle data "],
["clean-column-names.html", "Clean column names", " Clean column names You might have read that column names should not include spaces or special characters. Inevitably, you’ll encounter data that is not so tidy. For example: To tidy it up, you can use the clean_names() function from the janitor package: janitor::clean_names(df) "],
["fix-excel-dates.html", "Fix Excel dates", " Fix Excel dates Do you see something that looks like 43134, 43135, 43136 even though the column header is Dates? You may have encountered an Excel date/time problem. To fix it, the janitor package has a handy function: janitor::excel_numeric_to_date(c(43134, 43135, 43136)) Make sure you check that the date makes sense! "],
["solr-queries.html", "Solr queries ", " Solr queries "],
["construct-a-query.html", "Construct a query", " Construct a query Each Solr query is comprised of a number of parameters. These are like arguments to a function in R, but they are entered as parts of a URL. The most common parameters are: q: The query. This is like subset or dplyr::filter in R fl: What fields are returned for the documents that match your query (q). If not set, all fields are returned. rows: The maximum number of documents to return. Solr will truncate your result if the result size is greater than rows. sort: Sorts the result by the values in the given Solr field (e.g., sort by date uploaded) The query (‘q’) parameter uses a syntax that looks like field:value, where field is one of the Solr fields and value is an expression. The expression can match a specific value exactly, e.g., q=identifier:arctic-data.7747.1 or q=identifier:&quot;doi:10.5065/D60P0X4S&quot;, which finds the Solr document for a specific Object by PID (identifier). In the second example, the DOI PID is surrounded in double quotes. This is because Solr has reserved characters, of which : is one, so we have to help Solr by surrounding values with reserved characters in them in quotes or escaping them. To view the list of query-able parameters on the Arctic Data Center and their descriptions, you can visit https://arcticdata.io/metacat/d1/mn/v2/query/solr. "],
["example-solr-queries.html", "Example Solr queries", " Example Solr queries Find everything result &lt;- dataone::query(mn, list(q=&quot;*:*&quot;, fl=&quot;*&quot;, rows=&quot;5&quot;), as = &quot;data.frame&quot;) Query a wildcard expression #find any id that starts with arctic-data.6 result &lt;- dataone::query(mn, list(q=&quot;id:arctic-data.6*&quot;, fl=&quot;*&quot;, rows=&quot;5&quot;), as = &quot;data.frame&quot;) Query multiple fields result &lt;- dataone::query(mn, list(q=&quot;title:*soil*+AND+origin:*Ludwig*&quot;, fl=&quot;title&quot;, rows=&quot;5&quot;), as = &quot;data.frame&quot;) result &lt;- dataone::query(mn, list(q=&quot;title:*soil*+OR+origin:*Ludwig*&quot;, fl=&quot;title&quot;, rows=&quot;5&quot;), as = &quot;data.frame&quot;) Query multiple conditions within one field result &lt;- dataone::query(mn, list(q=&quot;title:(*soil*+AND+*carbon*)&quot;, fl=&quot;title&quot;, rows=&quot;5&quot;), as = &quot;data.frame&quot;) Use NOT in a query Just add - before a query parameter! result &lt;- dataone::query(mn, list(q=&quot;title:(*soil*+AND+-*carbon*)&quot;, fl=&quot;title&quot;, rows=&quot;5&quot;), as = &quot;data.frame&quot;) Query a coordinating node result &lt;- dataone::query(cn, list(q=&quot;title:*soil*+AND+origin:*Ludwig*&quot;, fl=&quot;title&quot;, rows=&quot;5&quot;), as = &quot;data.frame&quot;) Use facets All resource maps with &gt; 100 data objects that are not on the Arctic Data Center: https://cn.dataone.org/cn/v2/query/solr/?q=resourceMap:*+AND+-datasource:*ARCTIC*&amp;rows=0&amp;facet=true&amp;facet.field=resourceMap&amp;facet.mincount=100 "],
["query-solr-through-r.html", "Query Solr through R", " Query Solr through R Though it’s possible to query Solr directly through its HTTP API, we typically run our queries through R, for two main reasons: The result is returned in a more useful way to R without extra work on your part We can more easily pass our authentication token with the query Why does #2 matter? Well by default, all of those URLs above only returned publicly-readable Solr documents. If a private document matched any of those queries, Solr doesn’t give you any idea and acts like the non-public-readable documents don’t exist. So we must pass an authentication token to access non-public-readable content. This bit is crucial for working with the ADC, so you’ll very often want to use R instead of visiting those URLs in a web browser. cn &lt;- dataone::CNode(&quot;PROD&quot;) mn &lt;- dataone::getMNode(cn, &quot;urn:node:ARCTIC&quot;) # Set your token if you need/want! #string your parameters together like this: dataone::query(mn, &quot;q=title:*soil*&amp;fl=title&amp;rows=10&quot;) #or alternatively, list them out: dataone::query(mn, list(q=&quot;title:*soil*&quot;, fl=&quot;title&quot;, rows=&quot;10&quot;)) By default, query returns the result as a list, but a data.frame can be a more useful way to work with the result. To get a data.frame instead, just set the as argument to ‘data.frame’ to get a data.frame: dataone::query(mn, list(q=&quot;title:*soil*&quot;, fl=&quot;title&quot;, rows=&quot;10&quot;), as = &quot;data.frame&quot;) "],
["query-solr-via-a-browser.html", "Query Solr via a browser", " Query Solr via a browser Solr is queried via what’s called an HTTP API (Application Programming Interface). Practically, what this means it that you can execute a query in your browser by tacking a query onto a base URL. This is similar to the way Google handles your searches. If I search “soil science” in Google, for example, the URL becomes: https://www.google.com/search?q=soil+science&amp;oq=soil+science&amp;aqs=chrome.0.69i59.1350j0j1&amp;sourceid=chrome&amp;ie=UTF-8 If I break it down into pieces, I get: the base URL - https://www.google.com/search ?, after which the query parameters are listed the query - q=soil+science other parameters, which are separated by &amp; - oq=soil+science&amp;aqs=chrome.0.69i59.1350j0j1&amp;sourceid=chrome&amp;ie=UTF-8 Most of the time, you’ll query either the Arctic Data Center member node or the PROD coordinating node, which have the following base URLs: Arctic Data Center member node: https://arcticdata.io/metacat/d1/mn/v2/query/solr PROD coordinating node: https://cn.dataone.org/cn/v2/query/solr You can then append your query parameters to your base URL: https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q={QUERY}&amp;fl={FIELDS}&amp;rows={ROWS} Visit the base URL to see a list of fields Solr is storing for the objects it indexes. There is a large set of queryable fields, though not all types of objects will have values set for all of the possible fields because some fields do not make sense for some objects (e.g., title for a CSV). "],
["use-facets-1.html", "Use facets", " Use facets We can also summarize what’s in Solr with faceting, which lets us group Solr documents together and count them. This is like table in R. Faceting can do a query within a query, but more commonly it’s used to summarize unique values in a field. For example, we can find the unique format IDs on Data Objects: https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=*:*&amp;fq=formatType:DATA&amp;facet=true&amp;facet.field=formatId&amp;rows=0 To facet, we usually do a few things: Add the parameter facet=true Add the parameter facet.field={FIELD} with the field we want to facet (group) on Set rows=0 because we don’t care about the matched Solr documents Optionally specify fq={expression} which filters out Solr documents before faceting. In the above example, we have to do this to only count Data Objects. Without it, the facet result would include formatIDs for metadata and resource maps which we don’t want. Currently, the dataone::query() function does not support faceting, so you’ll have to run your queries as a URL. For additional ways to use faceting (such as pivot faceting), check out the Solr documentation. "],
["use-stats.html", "Use stats", " Use stats With stats, we can have Solr calculate statistics on numerical values (such as fileSize). https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=formatType:DATA&amp;stats=true&amp;stats.field=size&amp;rows=0 This query calculates a set of summary statistics for the size field on Data Objects that Solr has indexed. In this case, Solr’s size field indexes the fileSize field in the System Metadata for each Object in Metacat. "],
["more-resources.html", "More resources", " More resources Solr’s The Standard Query Parser docs (high level of detail) Another quick reference: https://wiki.apache.org/solr/SolrQuerySyntax http://www.solrtutorial.com/ "]
]
